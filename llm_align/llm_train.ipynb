{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys\n",
    "import math \n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import ipdb \n",
    "from typing import List, Dict, Union, Any, Tuple\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Import some Hugging Face Libraries\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optional for debugging, if you want to see the full tensor\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: You are using  cuda\n"
     ]
    }
   ],
   "source": [
    "#Training parameters\n",
    "batch_size = 1\n",
    "epochs = 3 # 3 is good, more overfits\n",
    "lr = 6e-5\n",
    "lr_warmup_steps = 100\n",
    "context = 1024\n",
    "alpha = 0.5 \n",
    "prompt_max_length = 512\n",
    "compile = False\n",
    "dtype = torch.bfloat16\n",
    "log_iter = 50\n",
    "\n",
    "# Hyperparameters\n",
    "dropout = 0.\n",
    "grad_clip = 1.0\n",
    "weight_decay = 0.0\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: You are using \", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: mistigri-heriveau (mistigri-heriveau-universit-toulouse-capitole). Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\MathisHERIVEAU\\Documents\\Apprentissage IA\\Deep Learning\\Module 7 - LLM - A to Z\\llm_align\\wandb\\run-20250221_090236-80mqoqf2</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation/runs/80mqoqf2' target=\"_blank\">LLama_knowledge_distillation_run_2025-02-21_09-02-33</a></strong> to <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation' target=\"_blank\">https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation/runs/80mqoqf2' target=\"_blank\">https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation/runs/80mqoqf2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging \n",
    "project_name = \"LLama_knowledge_distillation\"\n",
    "wandb_log = True \n",
    "wandb_project = project_name\n",
    "# ipdb.set_trace()\n",
    "wandb_run_name = f\"LLama_knowledge_distillation_run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found, loading from Hugging Face\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5189d70e2f6b4065afc34f593509c908",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/37635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159c70ec8c254d328b919b72f8aca720",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/37635 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.getcwd() \n",
    "dataset_name = 'MuskumPillerum/General-Knowledge'\n",
    "dataset_path = path + '\\data2\\General-Knowledge'\n",
    "tokenizer_path = path + '/tokenizers/tok16384'\n",
    "checkpoint_dir = path + '/models/'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Set the tokenizer parameters\n",
    "# tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n' + message['content'] + eos_token }}\\n{% endif %}{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>\\n' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "# Make padding token equal to the end of sentence token (wich has ID of 2 in our case)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    print(\"Dataset loaded from disk\")\n",
    "else:\n",
    "    print(\"Dataset not found, loading from Hugging Face\")\n",
    "    dataset = load_dataset(dataset_name, split='train')\n",
    "    \n",
    "    # Prétraitement pour transformer les questions et réponses en format utilisé pour l'entraînement\n",
    "    def preprocess_dataset(examples):\n",
    "        questions = examples['Question']\n",
    "        answers = examples['Answer']\n",
    "\n",
    "        # Vérification et conversion en string (évite les erreurs sur des valeurs nulles)\n",
    "        questions = [q if isinstance(q, str) else \"\" for q in questions]\n",
    "        answers = [a if isinstance(a, str) else \"\" for a in answers]\n",
    "\n",
    "        # Tokenisation\n",
    "        input_encodings = tokenizer(questions, truncation=True, padding=\"max_length\", max_length=context)\n",
    "        target_encodings = tokenizer(answers, truncation=True, padding=\"max_length\", max_length=context)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'],\n",
    "            'attention_mask': input_encodings['attention_mask'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "\n",
    "    # Appliquer la transformation\n",
    "    dataset = dataset.map(preprocess_dataset, batched=True, remove_columns=['Question', 'Answer'])\n",
    "    dataset.save_to_disk(dataset_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is Artificial Intelligence?</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[0]['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(42).train_test_split(test_size=0.05)\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=data_collector, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, collate_fn=data_collector, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "# print (tokenizer.decode(batch['positive_input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187.4176 M parameters\n"
     ]
    }
   ],
   "source": [
    "from llm import Llama, ModelArgs\n",
    "\n",
    "# Charger le checkpoint\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, 'newModelLLama_3.pt'))\n",
    "\n",
    "# Définir les arguments du modèle\n",
    "diviseurPerf = 4\n",
    "model_args = ModelArgs(\n",
    "    dim = 4096 // diviseurPerf, \n",
    "    n_layers = 32 // diviseurPerf,  \n",
    "    n_heads = 32 // diviseurPerf, \n",
    "    n_kv_heads = 8, \n",
    "    vocab_size = 128256 // diviseurPerf, \n",
    "    multiple_of = 256,  \n",
    "    ffn_dim_multiplier = None,\n",
    "    norm_eps = 1e-06, \n",
    "    rope_theta = 500000 // diviseurPerf, \n",
    "    max_seq_len = 8192 // diviseurPerf, \n",
    "    dropout = 0.1, \n",
    "    hidden_dim = 14336 // diviseurPerf,\n",
    "    attention_bias = True,\n",
    "    mlp_bias = True, \n",
    ")\n",
    "\n",
    "# Initialiser le modèle\n",
    "model = Llama(model_args)\n",
    "\n",
    "# Supprimer la clé \"config\" du checkpoint avant de charger les poids\n",
    "checkpoint.pop(\"config\", None)\n",
    "\n",
    "# Charger les poids du modèle\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Envoyer le modèle sur le bon device\n",
    "model = model.to(dtype=dtype, device=device)\n",
    "model.train()\n",
    "\n",
    "# Compiler si besoin\n",
    "if compile:\n",
    "    print('[INFO] Compiling model')\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Afficher le nombre de paramètres\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps: 107259\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-8, fused = device == 'cuda', weight_decay=weight_decay)\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "print(f\"num_training_steps: {num_training_steps}\")\n",
    "\n",
    "# Scheduler for lr: first 100 steps warmup, then decay\n",
    "def lr_lambda(step):\n",
    "    if step < lr_warmup_steps:\n",
    "        return float(step) / float(max(1, lr_warmup_steps))\n",
    "    else:\n",
    "        progress = float(step - lr_warmup_steps) / float(max(1, num_training_steps - lr_warmup_steps))\n",
    "        return max(0.0, math.cos(math.pi * float(0.5) * 2.0 * progress))\n",
    "    \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logps(prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "    mask = chosen_attention_mask[:,:-1] - prompt_attention_mask[:,1:]\n",
    "    per_token_lops = torch.gather(logits[:,:-1,:].log_softmax(-1), dim=2, \n",
    "                                  index=(mask * chosen_inputs[:,1:]).unsqueeze(2)).squeeze(2)\n",
    "    return torch.mul(per_token_lops, mask.to(dtype)).sum(dim=1).to(dtype) / mask.sum(dim=1).to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: [0/3] \tIteration: [0/35753] \tLoss: 5.969 \tTime left: 2h 2m 18s\n",
      "\tEpoch: [0/3] \tIteration: [50/35753] \tLoss: 3.422 \tTime left: 5h 52m 19s\n",
      "Training interrompu par l'utilisateur.\n",
      "Fin de l'entraînement, mémoire GPU libérée.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Variables pour le calcul du temps estimé\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for e in range(epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            \n",
    "            outputs, loss = model(batch['input_ids'], batch['labels'])\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Logging\n",
    "            if i % log_iter == 0:\n",
    "                # Temps écoulé depuis le début de l'entraînement\n",
    "                elapsed_time = time.time() - start_time\n",
    "                time_per_iter = elapsed_time / (i + 1)  # Temps moyen par itération\n",
    "                \n",
    "                # Estimation du temps restant\n",
    "                remaining_iters = len(train_loader) * (epochs - e - 1) + (len(train_loader) - i)\n",
    "                remaining_time = time_per_iter * remaining_iters\n",
    "                \n",
    "                # Affichage du temps estimé\n",
    "                print(f\"\\tEpoch: [{e}/{epochs}] \\tIteration: [{i}/{len(train_loader)}] \\tLoss: {loss.item():.3f} \"\n",
    "                      f\"\\tTime left: {remaining_time // 3600:.0f}h {(remaining_time % 3600) // 60:.0f}m \"\n",
    "                      f\"{remaining_time % 60:.0f}s\")\n",
    "                \n",
    "                # Logging dans fichier\n",
    "                with open(f\"{checkpoint_dir}/training_knowledge.log\", \"a\") as f:\n",
    "                    f.write(f\"Epoch: [{e}/{epochs}] Iteration: [{i}/{len(train_loader)}] Loss: {loss.item():.3f}\\n\")\n",
    "        \n",
    "        # Sauvegarde du modèle\n",
    "        sd = model.state_dict()\n",
    "        sd['config'] = model_args\n",
    "        torch.save(sd, os.path.join(checkpoint_dir, f'model_epoch_{e+1}.pt'))\n",
    "\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA Out of Memory! Essayez de réduire le batch size.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training interrompu par l'utilisateur.\")\n",
    "\n",
    "finally:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Fin de l'entraînement, mémoire GPU libérée.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kivy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
