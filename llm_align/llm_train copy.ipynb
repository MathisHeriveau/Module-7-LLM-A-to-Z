{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys\n",
    "import math \n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import ipdb \n",
    "from typing import List, Dict, Union, Any, Tuple\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Import some Hugging Face Libraries\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk, concatenate_datasets\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optional for debugging, if you want to see the full tensor\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: You are using  cuda\n"
     ]
    }
   ],
   "source": [
    "#Training parameters\n",
    "batch_size = 1\n",
    "epochs = 5 # 3 is good, more overfits\n",
    "lr = 6e-5\n",
    "lr_warmup_steps = 100\n",
    "context = 1024\n",
    "alpha = 0.5 \n",
    "prompt_max_length = 512\n",
    "compile = False\n",
    "dtype = torch.bfloat16\n",
    "log_iter = 50\n",
    "max_val_samples = 1000\n",
    "\n",
    "# Hyperparameters\n",
    "dropout = 0.\n",
    "grad_clip = 1.0\n",
    "weight_decay = 0.0\n",
    "\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: You are using \", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: mistigri-heriveau (mistigri-heriveau-universit-toulouse-capitole). Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\MathisHERIVEAU\\Documents\\Apprentissage IA\\Deep Learning\\Module 7 - LLM - A to Z\\llm_align\\wandb\\run-20250221_140435-vjrj9ghj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation/runs/vjrj9ghj' target=\"_blank\">LLama_knowledge_distillation_run_2025-02-21_14-04-33</a></strong> to <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation' target=\"_blank\">https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation/runs/vjrj9ghj' target=\"_blank\">https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/LLama_knowledge_distillation/runs/vjrj9ghj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging \n",
    "project_name = \"LLama_knowledge_distillation\"\n",
    "wandb_log = True \n",
    "wandb_project = project_name\n",
    "# ipdb.set_trace()\n",
    "wandb_run_name = f\"LLama_knowledge_distillation_run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "dataset_1 features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n",
      "dataset_2 features: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\n",
      "What is Artificial Intelligence?</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "Artificial Intelligence refers to the development of computer systems that can perform tasks that would typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\\n</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "\n",
      "\n",
      "\n",
      "What is the total work done on an object when it is moved upwards against gravity, considering both the change in kinetic energy and potential energy? Use the Work-Energy Theorem and the principle of conservation of mechanical energy to derive your answer.</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>\n",
      "## Step 1: Define the Work-Energy Theorem\n",
      "The Work-Energy Theorem states that the net work done on an object is equal to the change in its kinetic energy. Mathematically, this is represented as \\(W = \\Delta KE\\), where \\(W\\) is the net work done and \\(\\Delta KE\\) is the change in kinetic energy.\n",
      "\n",
      "## Step 2: Consider the Change in Potential Energy\n",
      "When an object is moved upwards against gravity, its potential energy increases. The change in potential energy (\\(\\Delta PE\\)) is given by \\(mgh\\), where \\(m\\) is the mass of the object, \\(g\\) is the acceleration due to gravity, and \\(h\\) is the height through which the object is lifted.\n",
      "\n",
      "## Step 3: Apply the Principle of Conservation of Mechanical Energy\n",
      "The principle of conservation of mechanical energy states that the total mechanical energy of an isolated system remains constant over time, unless acted upon by an external force. In this scenario, the external force is gravity. The total mechanical energy (\\(\n",
      "Dataset loaded from disk\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd() \n",
    "dataset_name = 'MuskumPillerum/General-Knowledge'\n",
    "tokenizer_path = path + '/tokenizers/tok16384'\n",
    "checkpoint_dir = path + '/models/'\n",
    "\n",
    "dataset_path_1 = path + '\\\\data2\\\\General-Knowledge'\n",
    "dataset_path_2 = path + '\\\\data2\\\\natural_reasoning'\n",
    "dataset_path = path + '\\\\data2\\\\other'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Set the tokenizer parameters\n",
    "# tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n' + message['content'] + eos_token }}\\n{% endif %}{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>\\n' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "# Make padding token equal to the end of sentence token (wich has ID of 2 in our case)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if os.path.exists(dataset_path_1) and os.path.exists(dataset_path_2):\n",
    "    dataset_1 = load_from_disk(dataset_path_1)\n",
    "    dataset_2 = load_from_disk(dataset_path_2)\n",
    "    \n",
    "    print(type(dataset_1))\n",
    "    print(type(dataset_2))\n",
    "    \n",
    "    dataset_2 = dataset_2.cast_column(\"labels\", dataset_1.features[\"labels\"])\n",
    "    dataset_2 = dataset_2.remove_columns([\"reference_answer\"])\n",
    "\n",
    "    print(\"dataset_1 features:\", dataset_1.features)\n",
    "    print(\"dataset_2 features:\", dataset_2.features)\n",
    "\n",
    "    \n",
    "    print(tokenizer.decode(dataset_1[0]['input_ids']))\n",
    "    print(tokenizer.decode(dataset_1[0]['labels']))\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    print(tokenizer.decode(dataset_2[0]['input_ids']))\n",
    "    print(tokenizer.decode(dataset_2[0]['labels']))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Concatenate the two datasets\n",
    "    dataset = concatenate_datasets([dataset_1, dataset_2])\n",
    "    \n",
    "    print(\"Dataset loaded from disk\")\n",
    "else:\n",
    "    print(\"Dataset not found, loading from Hugging Face\")\n",
    "    dataset = load_dataset(dataset_name, split='train')\n",
    "    \n",
    "    # Prétraitement pour transformer les questions et réponses en format utilisé pour l'entraînement\n",
    "    def preprocess_dataset(examples):\n",
    "        questions = examples['Question']\n",
    "        answers = examples['Answer']\n",
    "\n",
    "        # Vérification et conversion en string (évite les erreurs sur des valeurs nulles)\n",
    "        questions = [q if isinstance(q, str) else \"\" for q in questions]\n",
    "        answers = [a if isinstance(a, str) else \"\" for a in answers]\n",
    "\n",
    "        input_encodings = tokenizer(questions, truncation=True, padding=\"max_length\", max_length=context)\n",
    "        target_encodings = tokenizer(answers, truncation=True, padding=\"max_length\", max_length=context)\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_encodings['input_ids'],\n",
    "            'labels': target_encodings['input_ids']\n",
    "        }\n",
    "\n",
    "    # Appliquer la transformation\n",
    "    dataset = dataset.map(preprocess_dataset, batched=True, remove_columns=['Question', 'Answer'])\n",
    "    dataset.save_to_disk(dataset_path)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Artificial Intelligence refers to the development of computer systems that can perform tasks that would typically require human intelligence, such as visual perception, speech recognition, decision-making, and language translation.\\\\n</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[0]['input_ids'])\n",
    "tokenizer.decode(dataset[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(42).train_test_split(test_size=0.05)\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=data_collector, shuffle=True, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, collate_fn=data_collector, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "# print (tokenizer.decode(batch['positive_input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187.4176 M parameters\n"
     ]
    }
   ],
   "source": [
    "from llm import Llama, ModelArgs\n",
    "\n",
    "# Charger le checkpoint\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, 'newModelLLama_3.pt'))\n",
    "\n",
    "# Définir les arguments du modèle\n",
    "diviseurPerf = 4\n",
    "model_args = ModelArgs(\n",
    "    dim = 4096 // diviseurPerf, \n",
    "    n_layers = 32 // diviseurPerf,  \n",
    "    n_heads = 32 // diviseurPerf, \n",
    "    n_kv_heads = 8, \n",
    "    vocab_size = 128256 // diviseurPerf, \n",
    "    multiple_of = 256,  \n",
    "    ffn_dim_multiplier = None,\n",
    "    norm_eps = 1e-06, \n",
    "    rope_theta = 500000 // diviseurPerf, \n",
    "    max_seq_len = 8192 // diviseurPerf, \n",
    "    dropout = 0.1, \n",
    "    hidden_dim = 14336 // diviseurPerf,\n",
    "    attention_bias = True,\n",
    "    mlp_bias = True, \n",
    ")\n",
    "\n",
    "# Initialiser le modèle\n",
    "model = Llama(model_args)\n",
    "\n",
    "# Supprimer la clé \"config\" du checkpoint avant de charger les poids\n",
    "checkpoint.pop(\"config\", None)\n",
    "\n",
    "# Charger les poids du modèle\n",
    "model.load_state_dict(checkpoint)\n",
    "\n",
    "# Envoyer le modèle sur le bon device\n",
    "model = model.to(dtype=dtype, device=device)\n",
    "model.train()\n",
    "\n",
    "# Compiler si besoin\n",
    "if compile:\n",
    "    print('[INFO] Compiling model')\n",
    "    model = torch.compile(model)\n",
    "\n",
    "# Afficher le nombre de paramètres\n",
    "print(sum(p.numel() for p in model.parameters()) / 1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps: 5621430\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-8, fused = device == 'cuda', weight_decay=weight_decay)\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "print(f\"num_training_steps: {num_training_steps}\")\n",
    "\n",
    "# Scheduler for lr: first 100 steps warmup, then decay\n",
    "def lr_lambda(step):\n",
    "    if step < lr_warmup_steps:\n",
    "        return float(step) / float(max(1, lr_warmup_steps))\n",
    "    else:\n",
    "        progress = float(step - lr_warmup_steps) / float(max(1, num_training_steps - lr_warmup_steps))\n",
    "        return max(0.0, math.cos(math.pi * float(0.5) * 2.0 * progress))\n",
    "    \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Variables pour le calcul du temps estimé\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    for e in range(epochs):\n",
    "        for i, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            batch = {key: value.to(device) for key, value in batch.items()}\n",
    "            \n",
    "            # Entraînement du modèle\n",
    "            outputs, loss = model(batch['input_ids'], batch['labels'])\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Logging\n",
    "            if i % log_iter == 0:\n",
    "                # Temps écoulé depuis le début de l'entraînement\n",
    "                elapsed_time = time.time() - start_time\n",
    "                time_per_iter = elapsed_time / (i + 1)  # Temps moyen par itération\n",
    "                \n",
    "                # Estimation du temps restant\n",
    "                remaining_iters = len(train_loader) * (epochs - e - 1) + (len(train_loader) - i)\n",
    "                remaining_time = time_per_iter * remaining_iters\n",
    "                \n",
    "                # Affichage du temps estimé\n",
    "                print(f\"\\tEpoch: [{e}/{epochs}] \\tIteration: [{i}/{len(train_loader)}] \\tLoss: {loss.item():.3f} \"\n",
    "                      f\"\\tTime left: {remaining_time // 3600:.0f}h {(remaining_time % 3600) // 60:.0f}m \"\n",
    "                      f\"{remaining_time % 60:.0f}s\")\n",
    "                \n",
    "                # Logging dans fichier\n",
    "                with open(f\"{checkpoint_dir}/training_knowledge.log\", \"a\") as f:\n",
    "                    f.write(f\"Epoch: [{e}/{epochs}] Iteration: [{i}/{len(train_loader)}] Loss: {loss.item():.3f}\\n\")\n",
    "\n",
    "        # Validation à la fin de l'époque\n",
    "        model.eval()  # Passer le modèle en mode évaluation\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():  # Pas besoin de calculer les gradients pour la validation\n",
    "            for i, batch in enumerate(val_loader):\n",
    "                # Limiter le nombre d'exemples testés\n",
    "                if i >= max_val_samples:\n",
    "                    break\n",
    "                batch = {key: value.to(device) for key, value in batch.items()}\n",
    "                outputs, loss = model(batch['input_ids'], batch['labels'])\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Calcul de la perte moyenne de validation\n",
    "        val_loss /= max_val_samples\n",
    "\n",
    "        # Affichage de la perte de validation\n",
    "        print(f\"\\tEpoch: [{e}/{epochs}] \\tValidation Loss: {val_loss:.3f}\")\n",
    "        with open(f\"{checkpoint_dir}/training_knowledge.log\", \"a\") as f:\n",
    "            f.write(f\"Epoch: [{e}/{epochs}] Validation Loss: {val_loss:.3f}\\n\")\n",
    "            \n",
    "        model.train()\n",
    "        # Sauvegarde du modèle à la fin de l'époque\n",
    "        sd = model.state_dict()\n",
    "        sd['config'] = model_args\n",
    "        torch.save(sd, os.path.join(checkpoint_dir, f'LLama_knowledge_distillation_boost_{e+1}.pt'))\n",
    "\n",
    "except torch.cuda.OutOfMemoryError:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"CUDA Out of Memory! Essayez de réduire le batch size.\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training interrompu par l'utilisateur.\")\n",
    "\n",
    "finally:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Fin de l'entraînement, mémoire GPU libérée.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kivy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
