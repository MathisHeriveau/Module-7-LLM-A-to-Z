{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee4594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c7a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if you havent installed these libraries already outside of the notebook\n",
    "#!pip install -q ipdb\n",
    "#!pip install -q transformers\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/\n",
    "\n",
    "# Official Notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff85baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Feb 19 15:41:54 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.35                 Driver Version: 556.35         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A2000 8GB Lap...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   43C    P5              4W /   55W |     405MiB /   8192MiB |     31%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A     10844    C+G   ...iles\\obs-studio\\bin\\64bit\\obs64.exe      N/A      |\n",
      "|    0   N/A  N/A     20256    C+G   ...m Files\\Zscaler\\ZSATray\\ZSATray.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad59190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\n"
     ]
    }
   ],
   "source": [
    "# If you are running this online (for example at Google Colab), \n",
    "# make sure you have the support files on the same folder\n",
    "# Otherwise run this cell to download them\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created.\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"llm.py\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8ca543-b30c-49d0-95ff-166bedd5d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Pytorch\n",
    "import torch\n",
    "# Architecture\n",
    "import transformers\n",
    "# Import Llama based model\n",
    "from llm import Llama, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727a9850-348a-4bec-95b1-48fc61f3d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You use device:  cuda\n",
      "Mode::Using Orpo aligned model\n",
      "Using model aligned_model.pt\n"
     ]
    }
   ],
   "source": [
    "use_orpo = True  # use aligned checkpoint or not\n",
    "num_answers = 1\n",
    "temp = 0.5\n",
    "topk= 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"You use device: \", device)\n",
    "    \n",
    "tokenizer_path = \"tokenizers/tok16384\"\n",
    "model_path = \"./models/\"\n",
    "    \n",
    "if use_orpo==True:\n",
    "        model_inf, context= \"aligned_model.pt\", 1024  # ORPO is trained with context of 1024\n",
    "        print(\"Mode::Using Orpo aligned model\")\n",
    "else:\n",
    "        model_inf, context= \"base_model.pt\", 512  # The original was trained with context of 512\n",
    "        print(\"Mode::Using pretrained model without alignment\")\n",
    "\n",
    "print(f\"Using model {model_inf}\")\n",
    "   \n",
    "# Load model and extract config\n",
    "checkpoint = torch.load(os.path.join(model_path, model_inf), map_location=device)\n",
    "config = checkpoint.pop(\"config\")\n",
    "    \n",
    "# temporary fix if the model was trained and saved with torch.compile\n",
    "# The _orig_mod. prefix in your model's state dictionary keys is related to\n",
    "# how PyTorch handles compiled models, specifically when using the torch.compile function\n",
    "# When torch.compile is used, PyTorch might wrap the original model in a way that modifies\n",
    "# the names of its parameters and buffers. This wrapping can prepend a prefix like _orig_mod.\n",
    "# We remove those wrappings to make the checkpoint compatible with the non compiled version of the model\n",
    "new_dict = dict()\n",
    "for k in checkpoint.keys():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            #print(\"Removing _orig_mod wrapping\")\n",
    "            new_dict[k.replace(\"_orig_mod.\", \"\")] = checkpoint[k]\n",
    "        else:\n",
    "            new_dict[k] = checkpoint[k]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "548deb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 138.43 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Setup tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model_args = ModelArgs(\n",
    "        dim=config.hidden_size, \n",
    "        n_layers=config.num_hidden_layers, \n",
    "        n_heads=config.num_attention_heads, \n",
    "        n_kv_heads=config.num_key_value_heads, \n",
    "        vocab_size=config.vocab_size, \n",
    "        norm_eps=config.rms_norm_eps, \n",
    "        rope_theta=config.rope_theta,\n",
    "        max_seq_len=context, \n",
    "        dropout=config.attention_dropout, \n",
    "        hidden_dim=config.intermediate_size,\n",
    "        attention_bias=config.attention_bias,\n",
    "        mlp_bias=config.mlp_bias\n",
    "    )\n",
    "\n",
    "# Instantiate model, load parms, move to device\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(new_dict)\n",
    "if device.type == 'cuda':\n",
    "        model = model.to(torch.bfloat16)\n",
    "        model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1e6:.2f} M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56149fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded tokens: [5435, 5567]\n",
      "Vocabulary size: 16384\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Test input\")\n",
    "print(f\"Encoded tokens: {tokens}\")\n",
    "print(f\"Vocabulary size: {model.vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65c5efdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model vocabulary size: 16384\n",
      "Max token in input: 14161\n",
      "################## \n",
      "\n",
      "### Answer 1: \n",
      "How many apple we have in a world? \n",
      "  \n",
      "160 * 10 = 16\n",
      "\n",
      "# Answer\n",
      "\n",
      "160 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "How many apples in a world? \n",
      " \n",
      "Answer in English language.\n",
      "\n",
      "160 * 10 = 16\n",
      "\n",
      "# Answer\n",
      "\n",
      "160 \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Considering the number of apples in a world, how many apples would there be in a world with a population of 7 billion people? \n",
      " \n",
      "Certainly! The number of apples in a world with a population of 7 billion people is 4 billion. \n",
      " \n",
      "What are the potential consequences of the global population growth rate (GPGR) on the global food supply? \n",
      " \n",
      "The potential consequences of the population growth rate on the global food supply are vast. The population growth rate would cause a significant reduction in food production and consumption. This would result in a decrease in the availability of fresh, nutrient-dense foods. The reduction in the availability of certain essential nutrients could\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    qs = input(\"Enter text (q to quit) >>> \")\n",
    "    if qs == \"\":\n",
    "        continue\n",
    "    if qs == 'q':\n",
    "        break\n",
    "\n",
    "    # we activate chat template only for ORPO model because it was trained with it\n",
    "    if use_orpo:\n",
    "        qs = f\"<s> <|user|>\\n{qs}</s>\\n<s> <|assistant|> \"\n",
    "\n",
    "    x = tokenizer.encode(qs)\n",
    "    x = torch.tensor(x, dtype=torch.long, device=device)[None, ...]\n",
    "    \n",
    "    print(f\"Model vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Max token in input: {x.max()}\")\n",
    "\n",
    "\n",
    "    for ans in range(num_answers):\n",
    "        with torch.no_grad():\n",
    "            y = model.generate(\n",
    "                x, \n",
    "                max_new_tokens=256, \n",
    "                temperature=temp, \n",
    "                top_k=topk\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)   \n",
    "\n",
    "        output = model.clean_response(response)\n",
    "\n",
    "        print(\"################## \\n\")\n",
    "        print(f\"### Answer {ans+1}: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
