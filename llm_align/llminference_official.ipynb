{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cee4594a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### For GOOGLE COLAB and similar platform Users:\n",
    "#### Make sure to select a GPU in the online platform. Don't run this code with a CPU (it will be too slow)\n",
    "\n",
    "# If you are running this code locally, your GPU should be selected automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7c7a620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if you havent installed these libraries already outside of the notebook\n",
    "#!pip install -q ipdb\n",
    "#!pip install -q transformers\n",
    "\n",
    "# And if you are not in Google Colab and you didn't yet install Pytorch, make sure to do it:\n",
    "# find the ideal pytorch installation command at https://pytorch.org/get-started/locally/\n",
    "\n",
    "# Official Notebook #vj30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ff85baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Feb 21 08:37:26 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 556.35                 Driver Version: 556.35         CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                  Driver-Model | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A2000 8GB Lap...  WDDM  |   00000000:01:00.0  On |                  N/A |\n",
      "| N/A   44C    P8              4W /   49W |    2383MiB /   8192MiB |     38%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      8292      C   ...EAU\\.conda\\envs\\kivy_env\\python.exe      N/A      |\n",
      "|    0   N/A  N/A     20256    C+G   ...m Files\\Zscaler\\ZSATray\\ZSATray.exe      N/A      |\n",
      "|    0   N/A  N/A     23628      C   ...EAU\\.conda\\envs\\kivy_env\\python.exe      N/A      |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# You can use this command to view information about your GPU and the amount of free memory it has\n",
    "# Make sure that you have at last 4GB of free GPU memory to do this course\n",
    "!nvidia-smi \n",
    "# If you are using Google Colab or a similar online platform, make sure to select a GPU in the menus\n",
    "# In Google colab, at the moment the option is within the Runtime menus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad59190e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\n"
     ]
    }
   ],
   "source": [
    "# If you are running this online (for example at Google Colab), \n",
    "# make sure you have the support files on the same folder\n",
    "# Otherwise run this cell to download them\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created.\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"llm.py\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d8ca543-b30c-49d0-95ff-166bedd5d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Pytorch\n",
    "import torch\n",
    "# Architecture\n",
    "import transformers\n",
    "# Import Llama based model\n",
    "from llm import Llama, ModelArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "727a9850-348a-4bec-95b1-48fc61f3d749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You use device:  cuda\n",
      "Mode::Using Orpo aligned model\n",
      "Using model newModelLLama_3.pt\n"
     ]
    }
   ],
   "source": [
    "use_orpo = True  # use aligned checkpoint or not\n",
    "num_answers = 1\n",
    "temp = 0.5\n",
    "topk= 50\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"You use device: \", device)\n",
    "    \n",
    "tokenizer_path = \"tokenizers/tok16384\"\n",
    "model_path = \"./models/\"\n",
    "    \n",
    "if use_orpo==True:\n",
    "        model_inf, context= \"newModelLLama_3.pt\", 1024  # ORPO is trained with context of 1024\n",
    "        print(\"Mode::Using Orpo aligned model\")\n",
    "else:\n",
    "        model_inf, context= \"base_model.pt\", 512  # The original was trained with context of 512\n",
    "        print(\"Mode::Using pretrained model without alignment\")\n",
    "\n",
    "print(f\"Using model {model_inf}\")\n",
    "   \n",
    "# Load model and extract config\n",
    "checkpoint = torch.load(os.path.join(model_path, model_inf), map_location=device)\n",
    "config = checkpoint.pop(\"config\")\n",
    "    \n",
    "# temporary fix if the model was trained and saved with torch.compile\n",
    "# The _orig_mod. prefix in your model's state dictionary keys is related to\n",
    "# how PyTorch handles compiled models, specifically when using the torch.compile function\n",
    "# When torch.compile is used, PyTorch might wrap the original model in a way that modifies\n",
    "# the names of its parameters and buffers. This wrapping can prepend a prefix like _orig_mod.\n",
    "# We remove those wrappings to make the checkpoint compatible with the non compiled version of the model\n",
    "new_dict = dict()\n",
    "for k in checkpoint.keys():\n",
    "        if k.startswith(\"_orig_mod.\"):\n",
    "            #print(\"Removing _orig_mod wrapping\")\n",
    "            new_dict[k.replace(\"_orig_mod.\", \"\")] = checkpoint[k]\n",
    "        else:\n",
    "            new_dict[k] = checkpoint[k]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548deb7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model size: 187.42 M parameters\n"
     ]
    }
   ],
   "source": [
    "# Setup tokenizer\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "diviseurPerf = 4\n",
    "model_args = ModelArgs(\n",
    "    dim = 4096//diviseurPerf, \n",
    "    n_layers = 32//diviseurPerf,  \n",
    "    n_heads = 32//diviseurPerf, \n",
    "    n_kv_heads =  8, \n",
    "    vocab_size = 128256//diviseurPerf, \n",
    "    multiple_of = 256,  \n",
    "    ffn_dim_multiplier = None,\n",
    "    norm_eps = 1e-06, \n",
    "    rope_theta = 500000//diviseurPerf, \n",
    "    max_seq_len = 8192//diviseurPerf, \n",
    "    dropout = 0.1, \n",
    "    hidden_dim = 14336//diviseurPerf,\n",
    "    attention_bias = True,\n",
    "    mlp_bias = True, \n",
    ")\n",
    "\n",
    "\n",
    "# Instantiate model, load parms, move to device\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(new_dict)\n",
    "if device.type == 'cuda':\n",
    "        model = model.to(torch.bfloat16)\n",
    "        model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "print(f\"Model size: {model_size/1e6:.2f} M parameters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56149fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded tokens: [5435, 5567]\n",
      "Vocabulary size: 32064\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.encode(\"Test input\")\n",
    "print(f\"Encoded tokens: {tokens}\")\n",
    "print(f\"Vocabulary size: {model.vocab_size}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65c5efdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model vocabulary size: 16384\n",
      "Max token in input: 9948\n",
      "################## \n",
      "\n",
      "### Answer 1: \n",
      "hi \n",
      "  \n",
      "\n",
      "To find the sum of the sum of the sum of the sum of a sum of the sum of the sum of the sum, I can use the sum of the sum of the sum of the same as the sum of the sum of the sum of the sum is the sum of the sum of the sum of the sum of the sum of the sum is the sum of the sum of the sum of the sum of the sum is 2 times the sum of the sum of the sum of the sum is 5 + 2 = 2 + 2 + 2 + 3 = 1 + 2 + 2 + 3 + 3 + 2 + 2 = 10 + 2 + 2 + 2 + 2 + 3 + 2 + 3 + 2 + 3 + 3 + 6 + 2 + 2 + 2 + 2 + 2 = 3 + 10 + 3 + 1 + 2 + 5 = 3 + 1 = 5 + 5 + 3 = 3 + 3 + 3 + 2 + 3 = 16 + 2 + 4 + 2 + 5 = 3 + 3 + 2 + 3 + sides, I can plug in the sum of an integers in the sum of the sum is 2 + 2 + 2 + 3 = 2 + 3 + 2 + 2 + 2 + 2 + 3 + 2 = 2 +\n",
      "Model vocabulary size: 16384\n",
      "Max token in input: 9948\n",
      "################## \n",
      "\n",
      "### Answer 1: \n",
      "Give me an text from your mind \n",
      "   <</(->\n",
      "\n",
      "Then, I'd be happy to help you for your product to use the following steps:\n",
      "\n",
      "1. **Re the following steps:\n",
      "\n",
      "```\n",
      "</\n",
      "```\n",
      "\n",
      "                                                                                                                                                                                                                 \n",
      "Model vocabulary size: 16384\n",
      "Max token in input: 9948\n",
      "################## \n",
      "\n",
      "### Answer 1: \n",
      "Once oppon a time \n",
      "   <</  </(x) = 2x < 0) = 2x$.\n",
      "Now I can use the value of the value of the expression to find the expression and the equation of the equation and simplify the equation by the equation and simplify.\n",
      "I can use the equation to solve for x and get x = 3x + x = x + 4x + x + y = 2x + b + x = 4x + 2x + x = 2x + y = 2x + - 5 = 2x + 2 + 2x = 2$, I get \\cdot 2x = - 2x + 2x + 2 + 2x + 2 = 12 + 3x + 4x + 2x = 2x + 3x + 2x + 3x = -1 + 2x = 3x + 2x + 2x + 2 = - 3x + x = 0$.\n",
      "# Answer\n",
      "\n",
      "3x + 2 + 2 = 2x + 3x + 2x + 3 = 3x + 2x + 2x + 2x + 3x + 2x + 2x = 2x + 2x + 2 = 2x + 2x + 2x = 2x + 6 + 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Interactive loop\n",
    "while True:\n",
    "    qs = input(\"Enter text (q to quit) >>> \")\n",
    "    if qs == \"\":\n",
    "        continue\n",
    "    if qs == 'q':\n",
    "        break\n",
    "\n",
    "    # we activate chat template only for ORPO model because it was trained with it\n",
    "    if use_orpo:\n",
    "        qs = f\"<s> <|user|>\\n{qs}</s>\\n<s> <|assistant|> \"\n",
    "\n",
    "    x = tokenizer.encode(qs)\n",
    "    x = torch.tensor(x, dtype=torch.long, device=device)[None, ...]\n",
    "    \n",
    "    print(f\"Model vocabulary size: {tokenizer.vocab_size}\")\n",
    "    print(f\"Max token in input: {x.max()}\")\n",
    "\n",
    "\n",
    "    for ans in range(num_answers):\n",
    "        with torch.no_grad():\n",
    "            y = model.generate(\n",
    "                x, \n",
    "                max_new_tokens=256, \n",
    "                temperature=temp, \n",
    "                top_k=topk\n",
    "            )\n",
    "\n",
    "        response = tokenizer.decode(y[0].tolist(), skip_special_tokens=True)   \n",
    "\n",
    "        output = model.clean_response(response)\n",
    "\n",
    "        print(\"################## \\n\")\n",
    "        print(f\"### Answer {ans+1}: {output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
