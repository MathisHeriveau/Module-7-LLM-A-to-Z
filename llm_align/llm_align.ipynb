{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\n"
     ]
    }
   ],
   "source": [
    "# If you are running this online (for example at Google Colab), \n",
    "# make sure you have the support files on the same folder\n",
    "# Otherwise run this cell to download them\n",
    "\n",
    "# NOTE: Downloading will take a while, be patient. You can refresh your folder from time to time to see when the files\n",
    "# have been created.\n",
    "\n",
    "import os, requests, zipfile, io \n",
    "\n",
    "files_url = \"https://ideami.com/llm_align\"\n",
    "\n",
    "# Downloading proceeds if we detect that one of the key files to download is not present\n",
    "if not os.path.exists(f\"llm.py\"):\n",
    "    print(\"Downloading files using Python\")\n",
    "    response = requests.get(files_url)\n",
    "    zipfile.ZipFile(io.BytesIO(response.content)).extractall(\".\")\n",
    "else:\n",
    "    print(\"you seem to have already downloaded the files. If you wish to re-download them, delete the llm.py file\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import os, sys\n",
    "import math \n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import ipdb \n",
    "from typing import List, Dict, Union, Any, Tuple\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Import some Hugging Face Libraries\n",
    "import transformers\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Optional for debugging, if you want to see the full tensor\n",
    "torch.set_printoptions(threshold=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: You are using  cuda\n"
     ]
    }
   ],
   "source": [
    "#Training parameters\n",
    "batch_size = 4 \n",
    "epochs = 3 # 3 is good, more overfits\n",
    "lr = 6e-5\n",
    "lr_warmup_steps = 100\n",
    "context = 1024\n",
    "alpha = 0.5 \n",
    "prompt_max_length = 512\n",
    "compile = False\n",
    "dtype = torch.bfloat16\n",
    "log_iter = 50\n",
    "\n",
    "# Hyperparameters\n",
    "dropout = 0.\n",
    "grad_clip = 1.0\n",
    "weight_decay = 0.0\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device: You are using \", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Currently logged in as: mistigri-heriveau (mistigri-heriveau-universit-toulouse-capitole). Use `wandb login --relogin` to force relogin\n",
      "wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\MathisHERIVEAU\\Documents\\Apprentissage IA\\Deep Learning\\Module 7 - LLM - A to Z\\llm_align\\wandb\\run-20250220_141407-p4cjr5ft</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/aligntest2/runs/p4cjr5ft' target=\"_blank\">aligntest2_run_2025-02-20_14-14-05</a></strong> to <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/aligntest2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/aligntest2' target=\"_blank\">https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/aligntest2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/aligntest2/runs/p4cjr5ft' target=\"_blank\">https://wandb.ai/mistigri-heriveau-universit-toulouse-capitole/aligntest2/runs/p4cjr5ft</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Logging \n",
    "project_name = \"aligntest2\"\n",
    "wandb_log = True \n",
    "wandb_project = project_name\n",
    "# ipdb.set_trace()\n",
    "wandb_run_name = f\"aligntest2_run_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "if wandb_log:\n",
    "    import wandb\n",
    "    wandb.init(project=wandb_project, name=wandb_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not found, loading from Hugging Face\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad79fe47ded04a90a192b6fa4e97bfed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/4 shards):   0%|          | 0/38541 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = os.getcwd() \n",
    "dataset_path = path + '\\data2\\orpo_dataset'\n",
    "dataset_name = 'mlabonne/orpo-dpo-mix-40k'\n",
    "tokenizer_path = path +'/tokenizers/tok16384'\n",
    "checkpoint_dir = path +'/models/'\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(tokenizer_path)\n",
    "\n",
    "# Set the tokenizer parameters\n",
    "tokenizer.chat_template = \"{% for message in messages %}{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n' + message['content'] + eos_token }}\\n{% endif %}{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>\\n' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "\n",
    "# Make padding token equal to the end of sentence token (wich has ID of 2 in our case)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "if os.path.exists(dataset_path):\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    print(\"Dataset loaded from disk\")\n",
    "else:\n",
    "    print(\"Dataset not found, loading from Hugging Face\")\n",
    "    dataset = load_dataset(dataset_name, split='all')\n",
    "    # Optional: Filter out the toxic-dpo-v0.2 dataset\n",
    "    dataset = dataset.filter(lambda x: x['source'] != \"toxic-dpo-v0.2\")\n",
    "    \n",
    "    def filter_dataset(examples):\n",
    "        prompt_lenght = tokenizer.apply_chat_template(examples['chosen'][:-1], tokenize=True, add_generation_prompt=True, return_tensors='pt').size(-1)\n",
    "        \n",
    "        if prompt_lenght < prompt_max_length:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    \n",
    "    def preprocess_dataset(example: Union[List, Dict]):\n",
    "        # ipdb.set_trace()\n",
    "        prompt = [tokenizer.apply_chat_template(item[:-1], tokenize=False, add_generation_prompt=True) for item in example['chosen']]\n",
    "        chosen = [tokenizer.apply_chat_template(item, tokenize=False) for item in example['chosen']]\n",
    "        rejected = [tokenizer.apply_chat_template(item, tokenize=False) for item in example['rejected']]\n",
    "        \n",
    "        inputs = tokenizer(prompt, max_length=context, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        pos_labels = tokenizer(chosen, max_length=context, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        neg_labels = tokenizer(rejected, max_length=context, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
    "        \n",
    "        inputs['positive_input_ids'] = pos_labels['input_ids']\n",
    "        inputs['positive_attention_mask'] = pos_labels['attention_mask']\n",
    "        \n",
    "        inputs['negative_input_ids'] = neg_labels['input_ids']\n",
    "        inputs['negative_attention_mask'] = neg_labels['attention_mask']\n",
    "        \n",
    "        return inputs\n",
    "    \n",
    "    dataset = dataset.filter(filter_dataset)\n",
    "    \n",
    "    \n",
    "    dataset = dataset.map(preprocess_dataset, batched = True, num_proc=1, remove_columns=dataset.column_names)\n",
    "    \n",
    "    dataset.save_to_disk(dataset_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|user|>\\nHow many colors are traditionally recognized in a visible spectrum or optical rainbow?</s> \\n<|assistant|>\\nTraditionally, a visible spectrum or optical rainbow is said to consist of seven colors. The order of these colors is typically remembered using the acronym ROYGBIV - Red, Orange, Yellow, Green, Blue, Indigo, and Violet. However, it is important to note that the division of the spectrum into these seven constituent colors is largely a human construct. In reality, a rainbow encompasses a continuous spectrum of colors which blend seamlessly into one another, from red light, which has longer wavelengths, to violet light, which has shorter wavelengths. The specific selection of seven colors originates from the work of Sir Isaac Newton, who chose to divide the spectrum into seven colors to correlate with the seven notes in a western major scale of music.</s> \\n<|user|>\\nExplain the scientific reasoning behind the continuous spectrum of colors in a rainbow.</s> \\n<|assistant|>\\nThe continuous spectrum of colors in a rainbow is due to the process of dispersion. When sunlight interacts with a raindrop, it is refracted, or bent, and then reflected off the inside surface of the raindrop. As it exits the raindrop, the light is refracted again. \\n\\nThe key point here is that the amount of refraction, or bending, depends on the wavelength of the light. In the visible light spectrum, red light has the longest wavelength and is bent the least, while violet light has the shortest wavelength and is bent the most. This difference in refraction causes the white sunlight to spread out into its constituent colors, a process known as dispersion.\\n\\nBecause this process is gradual and continuous, with each color blending into the next, the resulting spectrum is also continuous. There are no distinct boundaries between one color and the next in a rainbow. The colors we see are a result of our human perception and the way our brains categorize different wavelengths of light.</s> \\n<|user|>\\nCould you elaborate on how our human perception and brain categorize different wavelengths of light into distinct colors?</s> \\n<|assistant|>\\nCertainly. The human eye perceives color through specialized cells in the retina called cones. There are three types of cone cells, each sensitive to different ranges of wavelengths that correspond to short (S), medium (M), and long (L) wavelengths. The S-cones are most responsive to light that we perceive as blue, M-cones to green, and L-cones to red.\\n\\nWhen light enters the eye, it stimulates these cones to varying degrees depending on its wavelength. For example, light with a wavelength around 560-580 nm would stimulate the L-cones more than the M or S cones, and our brain would interpret this as the color yellow.\\n\\nThis system allows us to see a wide range of colors, even though each type of cone is only sensitive to a limited range of wavelengths. It\\'s the combined response from all three types of cones that gives us our perception of color. This is known as trichromatic color vision.\\n\\nFurthermore, our brains categorize these different wavelengths into distinct colors based on learned associations and cultural influences. For example, we are taught from a young age to identify certain ranges of wavelengths with specific color names, such as \"red\", \"orange\", \"yellow\", etc. This process of categorization allows us to make sense of the continuous spectrum of light wavelengths that we perceive.\\n\\nIt\\'s also worth noting that our perception of color can be influenced by other factors, such as lighting conditions, the presence of other colors nearby, and even individual differences in the structure and function of our eyes and brains.</s> \\n</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(dataset[0]['positive_input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(42).train_test_split(test_size=0.05)\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collector = transformers.DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, collate_fn=data_collector, shuffle=False, num_workers=0)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, collate_fn=data_collector, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "it = iter(train_loader)\n",
    "batch = next(it)\n",
    "# print (tokenizer.decode(batch['positive_input_ids'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138.431232 M parameters\n"
     ]
    }
   ],
   "source": [
    "from llm import Llama, ModelArgs\n",
    "\n",
    "checkpoint = torch.load(os.path.join(checkpoint_dir, 'base_model.pt'))\n",
    "config = checkpoint.pop(\"config\")\n",
    "\n",
    "model_args = ModelArgs(\n",
    "    dim=config.hidden_size, \n",
    "    n_layers=config.num_hidden_layers, \n",
    "    n_heads=config.num_attention_heads, \n",
    "    n_kv_heads=config.num_key_value_heads, \n",
    "    vocab_size=config.vocab_size, \n",
    "    norm_eps=config.rms_norm_eps, \n",
    "    rope_theta=config.rope_theta,\n",
    "    max_seq_len=context, \n",
    "    dropout=config.attention_dropout, \n",
    "    hidden_dim=config.intermediate_size,\n",
    "    attention_bias=config.attention_bias,\n",
    "    mlp_bias=config.mlp_bias\n",
    ")\n",
    "\n",
    "model = Llama(model_args)\n",
    "model.load_state_dict(checkpoint)\n",
    "model = model.to(dtype=dtype, device=device)\n",
    "model.train()\n",
    "\n",
    "if compile:\n",
    "    print('[INFO] Compiling model')\n",
    "    model = torch.compile(model)\n",
    "\n",
    "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_training_steps: 27462\n"
     ]
    }
   ],
   "source": [
    "# Optimizer\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, betas=(0.9, 0.98), eps=1e-8, fused = device == 'cuda', weight_decay=weight_decay)\n",
    "\n",
    "num_training_steps = len(train_loader) * epochs\n",
    "print(f\"num_training_steps: {num_training_steps}\")\n",
    "\n",
    "# Scheduler for lr: first 100 steps warmup, then decay\n",
    "def lr_lambda(step):\n",
    "    if step < lr_warmup_steps:\n",
    "        return float(step) / float(max(1, lr_warmup_steps))\n",
    "    else:\n",
    "        progress = float(step - lr_warmup_steps) / float(max(1, num_training_steps - lr_warmup_steps))\n",
    "        return max(0.0, math.cos(math.pi * float(0.5) * 2.0 * progress))\n",
    "    \n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_logps(prompt_attention_mask, chosen_inputs, chosen_attention_mask, logits):\n",
    "    mask = chosen_attention_mask[:,:-1] - prompt_attention_mask[:,1:]\n",
    "    per_token_lops = torch.gather(logits[:,:-1,:].log_softmax(-1), dim=2, \n",
    "                                  index=(mask * chosen_inputs[:,1:]).unsqueeze(2)).squeeze(2)\n",
    "    return torch.mul(per_token_lops, mask.to(dtype)).sum(dim=1).to(dtype) / mask.sum(dim=1).to(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9154 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0/3] Iteration: [0/9154] Loss: 2.922 Odds Ratio: 0.334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/9154 [00:53<33:55:11, 13.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training interrupted\n",
      "Training finished, GPU memory cleaned\n"
     ]
    }
   ],
   "source": [
    "try :\n",
    "    for e in range (epochs):\n",
    "        for i, batch in tqdm(enumerate(train_loader), total=len(train_loader), dynamic_ncols=True):\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            batch['positive_input_ids'] = batch['positive_input_ids'].to(device)\n",
    "            batch['positive_attention_mask'] = batch['positive_attention_mask'].to(device)\n",
    "            batch['negative_input_ids'] = batch['negative_input_ids'].to(device)\n",
    "            batch['negative_attention_mask'] = batch['negative_attention_mask'].to(device)\n",
    "            batch['attention_mask'] = batch['attention_mask'].to(device)\n",
    "            \n",
    "            neg_labels = batch['negative_input_ids'].clone()\n",
    "            pos_labels = batch['positive_input_ids'].clone()\n",
    "            \n",
    "            # Calculate the loss\n",
    "            mask = batch['attention_mask'] * batch['positive_attention_mask'] # mask out the padding\n",
    "            pos_labels = pos_labels * mask.logical_not() \n",
    "            \n",
    "            pos_labels[pos_labels == 0] = tokenizer.pad_token_id           \n",
    "            pos_labels[pos_labels == tokenizer.eos_token_id] = -100\n",
    "            neg_labels[neg_labels == tokenizer.eos_token_id] = -100\n",
    "            \n",
    "            outputs_pos, loss_pos = model(batch['positive_input_ids'], pos_labels)\n",
    "            outputs_neg, _ = model(batch['negative_input_ids'], neg_labels)\n",
    "            \n",
    "            # Calulcate per token log probabilities, essential to calculate the ORPO LOG ODDS RATIO \n",
    "            pos_prob = compute_logps(\n",
    "                batch['attention_mask'], \n",
    "                batch['positive_input_ids'], \n",
    "                batch['positive_attention_mask'], \n",
    "                outputs_pos\n",
    "            )\n",
    "            neg_prob = compute_logps(\n",
    "                batch['attention_mask'],\n",
    "                batch['negative_input_ids'],\n",
    "                batch['negative_attention_mask'],\n",
    "                outputs_neg\n",
    "            )\n",
    "            \n",
    "            \n",
    "            # Calculate the ORPO odds ratio\n",
    "            log_odds = (pos_prob - neg_prob) - (torch.log(1 - torch.exp(pos_prob)) - torch.log(1 - torch.exp(neg_prob)))\n",
    "            sig_ratio = F.sigmoid(log_odds) # Sigmoid to get the ratio between 0 and 1\n",
    "            ratio = torch.log(sig_ratio)\n",
    "            \n",
    "            # Calculate the loss\n",
    "            loss = torch.mean(loss_pos - (alpha * ratio).mean()).to(dtype)\n",
    "            \n",
    "            # Logging \n",
    "            if i % log_iter == 0:\n",
    "                print(f\"Epoch: [{e}/{epochs}] Iteration: [{i}/{len(train_loader)}] Loss: {loss.item():.3f} Odds Ratio: {log_odds.mean().item():.3f}\")\n",
    "                if wandb_log:\n",
    "                    wandb.log({\"loss\": loss.item(),\n",
    "                               \"odds_ratio\": log_odds.mean().item(),\n",
    "                               \"lr\" : scheduler.get_last_lr()[0],\n",
    "                               \"epoch\": e,\n",
    "                               \"iteration\": i})\n",
    "                if torch.isnan(loss):\n",
    "                    print(\"Loss is NaN, breaking\")\n",
    "                    if wandb_log:\n",
    "                        wandb.finish()\n",
    "                    torch.cuda.empty_cache()\n",
    "                    sys.exit()\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "        # Save the model\n",
    "        sd = model.state_dict()\n",
    "        sd['config'] = config\n",
    "        torch.save(sd, os.path.join(checkpoint_dir, f'base_model_{e+1}.pt'))\n",
    "                        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Training interrupted\")\n",
    "    pass\n",
    "finally:\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training finished, GPU memory cleaned\")\n",
    "    pass\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kivy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
